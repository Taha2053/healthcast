import pandas as pd
import json
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# -------------------------------
# 1. Load dataset
# -------------------------------
df = pd.read_json("C:/Users/HP/Downloads/work/fitness-users.json")  # replace with your dataset file
df = df.drop(["join_date","preferred_workout_time"], axis=1)

# Example: add target columns (in your dataset, you should already have them)
foods = ["Oatmeal", "Eggs", "Smoothie", "Oatmeal", "Eggs"]
df["breakfast_food"] = (foods * (len(df)//len(foods) + 1))[:len(df)]

lunches = ["Chicken Salad", "Rice & Beans", "Grilled Fish", "Pasta", "Chicken Salad"]
df["lunch_food"] = (lunches * (len(df)//len(lunches) + 1))[:len(df)]

dinners = ["Grilled Salmon", "Steak", "Vegetable Soup", "Chicken Wrap", "Grilled Salmon"]
df["dinner_food"] = (dinners * (len(df)//len(dinners) + 1))[:len(df)]

print(f"Dataset shape: {df.shape}")
print(f"Sample data:\n{df.head()}")

# -------------------------------
# 2. Preprocess features
# -------------------------------
categorical_cols = ["gender", "fitness_level", "goals", "activity_level"]

# Encode categorical with "unknown" fallback
encoders = {}
for col in categorical_cols:
    df[col] = df[col].astype(str).fillna("unknown")
    le = LabelEncoder()
    le.fit(df[col].tolist() + ["unknown"])
    df[col] = le.transform(df[col])
    encoders[col] = le

# Scale numerical columns (XGBoost can handle raw features, but scaling often helps)
scaler = StandardScaler()
df[["age", "weight", "height", "bmi"]] = scaler.fit_transform(df[["age", "weight", "height", "bmi"]])

# Encode meal labels
food_encoders = {}
for meal_col in ["breakfast_food", "lunch_food", "dinner_food"]:
    le = LabelEncoder()
    df[meal_col] = df[meal_col].astype(str)
    le.fit(df[meal_col])
    df[meal_col] = le.transform(df[meal_col])
    food_encoders[meal_col] = le

print("\nFeature encoding completed!")

# -------------------------------
# 3. Train XGBoost models with hyperparameter tuning
# -------------------------------
X = df.drop(columns=["user_id", "breakfast_food", "lunch_food", "dinner_food"])
y_breakfast = df["breakfast_food"]
y_lunch = df["lunch_food"]
y_dinner = df["dinner_food"]

print(f"Feature matrix shape: {X.shape}")
print(f"Features: {list(X.columns)}")

# XGBoost parameters for hyperparameter tuning
xgb_params = {
    'max_depth': [3, 4, 5, 6],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

# For faster execution, use a smaller parameter grid
xgb_params_small = {
    'max_depth': [4, 6],
    'learning_rate': [0.1, 0.2],
    'n_estimators': [100, 200],
    'subsample': [0.8, 1.0]
}

models = {}
best_params = {}

meal_types = ['breakfast', 'lunch', 'dinner']
targets = [y_breakfast, y_lunch, y_dinner]

for meal_type, y in zip(meal_types, targets):
    print(f"\n=== Training {meal_type.upper()} Model ===")
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Create XGBoost classifier
    xgb_model = xgb.XGBClassifier(
        objective='multi:softprob',
        random_state=42,
        eval_metric='mlogloss'
    )
    
    # Perform Grid Search for hyperparameter tuning
    print("Performing hyperparameter tuning...")
    grid_search = GridSearchCV(
        estimator=xgb_model,
        param_grid=xgb_params_small,  # Use smaller grid for faster execution
        scoring='accuracy',
        cv=3,  # 3-fold cross-validation
        n_jobs=-1,
        verbose=0
    )
    
    grid_search.fit(X_train, y_train)
    
    # Get best model
    best_model = grid_search.best_estimator_
   
    
    # Evaluate on test set
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"{meal_type.capitalize()} Test Accuracy: {accuracy:.3f}")
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nTop 3 most important features for {meal_type}:")
    print(feature_importance.head(3).to_string(index=False))
    
    # Store model
    models[meal_type] = best_model

print(f"\n=== Model Training Complete ===")
print("Best parameters for each meal:")
for meal, params in best_params.items():
    print(f"{meal}: {params}")

# -------------------------------
# 4. Enhanced Food dictionary with portions and nutrition
# -------------------------------
food_library = {
    "Oatmeal": [("Oatmeal", "50g"), ("Banana", "1 piece"), ("Milk", "200ml")],
    "Eggs": [("Eggs", "2 pieces"), ("Toast", "2 slices"), ("Orange Juice", "250ml")],
    "Smoothie": [("Smoothie", "300ml"), ("Granola", "30g")],
    "Chicken Salad": [("Chicken Breast", "120g"), ("Lettuce", "50g"), ("Olive Oil", "10ml")],
    "Rice & Beans": [("Rice", "100g"), ("Beans", "80g"), ("Avocado", "50g")],
    "Grilled Fish": [("Fish", "150g"), ("Quinoa", "100g"), ("Spinach", "60g")],
    "Pasta": [("Pasta", "120g"), ("Tomato Sauce", "80g"), ("Parmesan", "20g")],
    "Grilled Salmon": [("Salmon", "150g"), ("Asparagus", "80g"), ("Sweet Potato", "100g")],
    "Steak": [("Steak", "180g"), ("Mashed Potatoes", "100g"), ("Green Beans", "70g")],
    "Vegetable Soup": [("Soup", "300ml"), ("Bread", "1 slice")],
    "Chicken Wrap": [("Chicken", "100g"), ("Tortilla", "1 piece"), ("Lettuce", "40g")]
}

def expand_meal(pred):
    """Expand predicted dish into full meal with portions"""
    return [{"food": food, "amount": amount} for food, amount in food_library.get(pred, [(pred, "1 serving")])]

def get_prediction_probabilities(user_features, meal_type):
    """Get prediction probabilities for better insights"""
    model = models[meal_type]
    probabilities = model.predict_proba(user_features)[0]
    classes = food_encoders[f"{meal_type}_food"].classes_
    
    # Create probability dictionary
    prob_dict = dict(zip(classes, probabilities))
    # Sort by probability
    sorted_probs = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_probs

# -------------------------------
# 5. Generate enhanced meal plan with probabilities
# -------------------------------
def generate_meal_plan_with_alternatives(user_data, show_alternatives=True):
    """Generate meal plan with alternative suggestions"""
    user_df = pd.DataFrame([user_data])
    
    # Encode safely with fallback
    for col in categorical_cols:
        user_df[col] = user_df[col].astype(str)
        user_df[col] = user_df[col].apply(lambda x: x if x in encoders[col].classes_ else "unknown")
        user_df[col] = encoders[col].transform(user_df[col])
    
    # Scale numerics
    user_df[["age", "weight", "height", "bmi"]] = scaler.transform(user_df[["age", "weight", "height", "bmi"]])
    X_user = user_df.drop(columns=["user_id"])
    
    meal_plan = {"meal_plan": []}
    
    for meal_type in ['breakfast', 'lunch', 'dinner']:
        # Main prediction
        pred_encoded = models[meal_type].predict(X_user)[0]
        main_pred = food_encoders[f"{meal_type}_food"].inverse_transform([pred_encoded])[0]
        
        meal_info = {
            "meal": meal_type,
            "recommended": main_pred,
            "foods": expand_meal(main_pred)
        }
        
        # Add alternatives if requested
        if show_alternatives:
            probabilities = get_prediction_probabilities(X_user, meal_type)
            alternatives = []
            
            # Get top 3 alternatives (excluding the main prediction)
            for food, prob in probabilities[1:4]:
                alternatives.append({
                    "dish": food,
                    "confidence": f"{prob:.2%}",
                    "foods": expand_meal(food)
                })
            
            meal_info["alternatives"] = alternatives
            meal_info["main_confidence"] = f"{probabilities[0][1]:.2%}"
        
        meal_plan["meal_plan"].append(meal_info)
    
    return meal_plan

# Test with multiple users
test_users = [
    {
        "user_id": "user_test",
        "age": 30,
        "weight": 70,
        "height": 175,
        "bmi": 22.9,
        "fitness_level": "beginner",
        "goals": "weight_loss",
        "gender": "male",
        "activity_level": "moderate"
    },
    {
        "user_id": "eya",
        "age": 22,
        "weight": 55,
        "height": 165,
        "bmi": 20.2,
        "fitness_level": "beginner",
        "goals": "weight_loss",
        "gender": "female",
        "activity_level": "moderate"
    }
]

print(f"\n=== Generating Meal Plans ===")

for user in test_users:
    print(f"\nGenerating meal plan for {user['user_id']}...")
    
    # Generate meal plan with alternatives
    meal_plan = generate_meal_plan_with_alternatives(user, show_alternatives=True)
    
    # Save JSON
    filename = f"meal_plan_{user['user_id']}_xgboost.json"
    with open(filename, "w") as f:
        json.dump(meal_plan, f, indent=2)
    
    print(f"Meal plan saved to: {filename}")
    
    # Print main recommendations
    print(f"\nMain recommendations for {user['user_id']}:")
    for meal in meal_plan["meal_plan"]:
        print(f"  {meal['meal'].capitalize()}: {meal['recommended']} (confidence: {meal.get('main_confidence', 'N/A')})")

# Print detailed meal plan for first user
print(f"\n=== Detailed Meal Plan for {test_users[0]['user_id']} ===")
detailed_plan = generate_meal_plan_with_alternatives(test_users[0], show_alternatives=True)
print(json.dumps(detailed_plan, indent=2))

print(f"\n=== XGBoost Model Training and Prediction Complete! ===")